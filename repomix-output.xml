This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
app/api/convert-mp4/route.js
app/api/generate-samples/route.js
app/api/generate-scenario/route.js
app/api/transcribe/route.js
app/api/tts/route.js
app/chatgpt/page.jsx
app/client-page.jsx
app/favicon.ico
app/globals.css
app/layout.jsx
app/page.jsx
desktop.ini
eslint.config.mjs
next.config.js
nixpacks.toml
package.json
postcss.config.js
public/audiograffiti-logo.png
public/characters/boomer.png
public/characters/brittany.png
public/characters/chuck.png
public/characters/coral.png
public/characters/kaitlyn.png
public/characters/max.png
public/characters/randy.png
public/characters/sage.png
public/characters/shawn.png
public/file.svg
public/globe.svg
public/next.svg
public/scenaryoze-logo.png
public/vercel.svg
public/window.svg
railway.json
README.md
tailwind.config.js
test_shawn.mp3
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/api/generate-samples/route.js">
// Generate voice samples for all 9 Scenaryoze characters
// Run with: node generate-character-samples.js

const fs = require('fs');
const path = require('path');

const FISH_AUDIO_API_KEY = process.env.FISH_AUDIO_API_KEY;

if (!FISH_AUDIO_API_KEY) {
  console.error('ERROR: FISH_AUDIO_API_KEY environment variable not set');
  process.exit(1);
}

// Character voice mappings
const CHARACTERS = {
  shawn: {
    id: '536d3a5e000945adb7038665781a4aca',
    name: 'Shawn',
    intro: "Hi, I'm Shawn. I bring professionalism and clarity to every training scenario."
  },
  chuck: {
    id: 'ccbc13d6002a46b7883f607fd8fe0516',
    name: 'Chuck',
    intro: "Hey there, I'm Chuck. I make complex topics easy to understand and engaging."
  },
  max: {
    id: '37a48fabcd8241ab9b69d8675fb1fe13',
    name: 'Max',
    intro: "Hello, I'm Max. I excel at delivering confident, authoritative training content."
  },
  boomer: {
    id: 'ba24f05b17644498adb77243afd11dd9',
    name: 'Boomer',
    intro: "Greetings, I'm Boomer. I bring experience and wisdom to leadership scenarios."
  },
  brittany: {
    id: '2a9605eeafe84974b5b20628d42c0060',
    name: 'Brittany',
    intro: "Hi everyone, I'm Brittany! I specialize in friendly, approachable customer service training."
  },
  kaitlyn: {
    id: 'da8ae28bb18d4a1ca55eccf096f4c8da',
    name: 'Kaitlyn',
    intro: "Hello, I'm Kaitlyn. I bring warmth and authenticity to every interaction."
  },
  sage: {
    id: '933563129e564b19a115bedd57b7406a',
    name: 'Sage',
    intro: "Hi there, I'm Sage. I create calm, professional environments for effective learning."
  },
  randy: {
    id: 'bf322df2096a46f18c579d0baa36f41d',
    name: 'Randy',
    intro: "Hey, I'm Randy. I deliver energetic, engaging training that keeps teams motivated."
  },
  coral: {
    id: 'e107ce68d2a64e928c3a674781ce9d56',
    name: 'Coral',
    intro: "Hello! I'm Coral. I bring enthusiasm and positivity to customer-facing scenarios."
  }
};

async function generateSample(character, voiceId, text) {
  console.log(`Generating sample for ${character}...`);
  
  try {
    const response = await fetch('https://api.fish.audio/v1/tts', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${FISH_AUDIO_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        text: text,
        reference_id: voiceId,
        model: 's1',
        format: 'mp3',
        normalize: false,
        latency: 'normal'
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Fish Audio API error for ${character}: ${response.status} - ${errorText}`);
    }

    const audioBuffer = Buffer.from(await response.arrayBuffer());
    const outputPath = path.join(__dirname, `sample-${character}.mp3`);
    fs.writeFileSync(outputPath, audioBuffer);
    
    console.log(`‚úÖ ${character}: ${audioBuffer.length} bytes ‚Üí ${outputPath}`);
    return true;
  } catch (error) {
    console.error(`‚ùå Failed to generate ${character}:`, error.message);
    return false;
  }
}

async function generateAllSamples() {
  console.log('üéôÔ∏è  Generating voice samples for all 9 characters...\n');
  
  const results = [];
  
  for (const [key, data] of Object.entries(CHARACTERS)) {
    const success = await generateSample(data.name, data.id, data.intro);
    results.push({ character: data.name, success });
    
    // Small delay between requests to avoid rate limiting
    await new Promise(resolve => setTimeout(resolve, 500));
  }
  
  console.log('\nüìä Summary:');
  console.log(`‚úÖ Successful: ${results.filter(r => r.success).length}`);
  console.log(`‚ùå Failed: ${results.filter(r => !r.success).length}`);
  
  if (results.every(r => r.success)) {
    console.log('\nüéâ All character samples generated successfully!');
    console.log('üìÅ Files saved to current directory as sample-*.mp3');
  }
}

// Run the generator
generateAllSamples().catch(console.error);
</file>

<file path="app/api/generate-scenario/route.js">
import { NextResponse } from "next/server";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(req) {
  try {
    const { problemDescription } = await req.json();
    
    if (!problemDescription?.trim()) {
      return NextResponse.json(
        { error: "Problem description is required" },
        { status: 400 }
      );
    }

    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: `You are an expert L&D professional creating realistic 3-minute training scenarios.

RULES:
- Generate a 2-person dialogue (450-600 words total)
- Use ONLY these characters: SHAWN and BRITTANY
- Format each line as: [CHARACTER]: dialogue text
- SHAWN speaks first
- Natural conversation with realistic speech patterns
- Focus on ONE clear learning objective
- End with successful resolution

OUTPUT FORMAT:
Title: [Clear Title]

[SHAWN]: [First line]
[BRITTANY]: [Response]
[SHAWN]: [Next line]
...`
        },
        {
          role: "user",
          content: `Create a training scenario for this problem:\n\n"${problemDescription}"`
        }
      ],
      temperature: 0.8,
      max_tokens: 1500,
    });

    const rawScenario = completion.choices[0].message.content;
    
    const lines = rawScenario.split('\n').filter(line => line.trim());
    const titleLine = lines.find(line => line.toLowerCase().startsWith('title:'));
    const title = titleLine 
      ? titleLine.replace(/^title:\s*/i, '').trim() 
      : 'Training Scenario';
    
    const dialogue = [];
    
    for (const line of lines) {
      const match = line.match(/^\[?([A-Z][A-Z\s\-]*)\]?:\s*(.+)$/);
      if (match) {
        const character = match[1].trim().toUpperCase();
        const text = match[2].trim();
        
        if ((character === 'SHAWN' || character === 'BRITTANY') && text.length > 0) {
          dialogue.push({
            character,
            text,
            voice: character.toLowerCase()
          });
        }
      }
    }

    if (dialogue.length < 6) {
      throw new Error('Generated scenario too short');
    }

    const wordCount = dialogue.reduce((sum, d) => sum + d.text.split(/\s+/).length, 0);

    const segments = dialogue.map(line => ({
      text: line.text,
      voice: line.voice
    }));

    return NextResponse.json({
      success: true,
      title,
      dialogue,
      segments,
      wordCount
    });

  } catch (error) {
    console.error('Scenario generation failed:', error);
    
    return NextResponse.json(
      { 
        success: false,
        error: error.message || 'Scenario generation failed'
      },
      { status: 500 }
    );
  }
}
</file>

<file path="desktop.ini">
[LocalizedFileNames]
XY - Novel Chat on GPT4o.docx=@XY - Novel Chat on GPT4o.docx,0
XY-A Genetic Dystopia.docx=@XY-A Genetic Dystopia.docx,0
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path="app/client-page.jsx">
'use client';

import React, { useEffect, useMemo, useRef, useState } from 'react';

/* ============================ CONSTANTS ============================ */
const FORMATS = {
  '4:3':  { width: 1080, height: 1080,  name: 'Square (1:1)' },
};

const FPS = 30;
const MAX_LINES = 5;
const MAX_WORDS_PER_SEGMENT = 18;
const DEFAULT_VOICE = 'brittany';
const VOICE_STORAGE_KEY = 'ag:lastVoice';

const PLAN_LIMITS = {
  free: { maxChars: 50000, displayName: 'Free' },
  pro: { maxChars: 50000, displayName: 'Pro' }
};

const PRESETS = [
  ['#0d1117', '#1f2937'],
  ['#111827', '#2563eb'],
  ['#1f2937', '#10b981'],
  ['#3b0764', '#f43f5e'],
  ['#0f172a', '#9333ea'],
  ['#7c2d12', '#d97706'],
  ['#0c4a6e', '#22d3ee'],
  ['#111827', '#f59e0b'],
  ['#0f172a', '#14b8a6'],
];

/* ============================== UTILS ============================== */

function splitWords(s) {
  return s.trim().replace(/\s+/g, ' ').split(' ').filter(Boolean);
}

function coalesceSegments(segments, minDur = 0.4) {
  const out = [];
  for (const seg of segments) {
    const last = out[out.length - 1];
    const dur = seg.end - seg.start;
    if (last && last.end - last.start < minDur && dur < minDur) {
      last.text = (last.text + ' ' + seg.text).trim();
      last.end = seg.end;
    } else {
      out.push({ ...seg });
    }
  }
  return out;
}

function tightenSegments(segs, maxWords = MAX_WORDS_PER_SEGMENT) {
  const out = [];
  for (const s of segs) {
    const words = splitWords(s.text);
    if (words.length <= maxWords) {
      out.push(s);
      continue;
    }
    const chunks = [];
    for (let i = 0; i < words.length; i += maxWords) {
      chunks.push(words.slice(i, i + maxWords).join(' '));
    }
    const per = (s.end - s.start) / chunks.length;
    for (let i = 0; i < chunks.length; i++) {
      out.push({
        start: s.start + i * per,
        end: s.start + (i + 1) * per,
        text: chunks[i],
      });
    }
  }
  return out;
}

function normalizeSegments(segs, totalDurGuess) {
  let out = tightenSegments(coalesceSegments(segs));
  out = out
    .map((s) => ({
      start: Math.max(0, s.start),
      end: Math.max(0, s.end),
      text: (s.text || '').trim(),
    }))
    .filter((s) => s.end > s.start)
    .sort((a, b) => a.start - b.start);

  for (let i = 1; i < out.length; i++) {
    if (out[i].start < out[i - 1].end) out[i].start = out[i - 1].end;
  }

  const last = out[out.length - 1];
  const total = Number.isFinite(totalDurGuess) && totalDurGuess > 0 ? totalDurGuess : last?.end ?? 0;
  if (last && total > 0) last.end = Math.max(last.end, total);
  return out;
}

/* ============= NEW: CHARACTER TAG PARSING ============= */

/**
 * Parse character-tagged script in format: [NAME]: dialogue
 * Returns: { lines: [{speaker, text}], characters: [unique names] }
 */
function parseCharacterScript(scriptText) {
  if (!scriptText || typeof scriptText !== 'string') {
    return { lines: [], characters: [] };
  }

  const lines = [];
  const characterSet = new Set();
  
  // Split by newlines and process each line
  const rawLines = scriptText.split('\n');
  
  for (const rawLine of rawLines) {
    const trimmed = rawLine.trim();
    if (!trimmed) continue; // Skip empty lines
    
    // Match pattern: [CHARACTER]:, CHARACTER:, or **CHARACTER:** dialogue text
    // Supports uppercase letters, spaces, hyphens in character names
    // Flexible format accepts brackets and/or bold markdown
    const match = trimmed.match(/^\*?\*?\[?([A-Z][A-Z\s\-]*)\]?\*?\*?:\s*(.+)$/i);
    
    if (match) {
      const speaker = match[1].trim().toUpperCase();
      const text = match[2].trim();
      
      if (speaker && text) {
        lines.push({ speaker, text });
        characterSet.add(speaker);
      }
    }
  }
  
  return {
    lines,
    characters: Array.from(characterSet).sort()
  };
}

/* ====================================================== */

// Voice mapping: Character names (backend handles ElevenLabs mapping)
const VOICE_API_MAPPING = {
  'shawn': 'shawn',
  'chuck': 'chuck',
  'max': 'max',
  'boomer': 'boomer',
  'brittany': 'brittany',
  'kaitlyn': 'kaitlyn',
  'sage': 'sage',
  'randy': 'randy',
  'coral': 'coral'
};

// Voice color themes for default character images (using custom names)
const VOICE_THEMES = {
  shawn: { bg: '#2563EB', name: 'Shawn', apiVoice: 'Brian' },
  chuck: { bg: '#3B82F6', name: 'Chuck', apiVoice: 'Chuck Clone 2' },
  brittany: { bg: '#8B5CF6', name: 'Brittany', apiVoice: 'Adeline' },
  kaitlyn: { bg: '#EC4899', name: 'Kaitlyn', apiVoice: 'Rachel' },
  boomer: { bg: '#1F2937', name: 'Boomer', apiVoice: 'James' },
  sage: { bg: '#10B981', name: 'Sage', apiVoice: 'Kaylin' },
  max: { bg: '#F59E0B', name: 'Max', apiVoice: 'Adam Stone' },
  randy: { bg: '#9CA3AF', name: 'Randy', apiVoice: 'Ryan' },
  coral: { bg: '#F97316', name: 'Coral', apiVoice: 'Nicole' },
};

/**
 * Load character image from public folder
 * Returns a promise that resolves to { url, img }
 */
function loadCharacterImage(voiceName) {
  return new Promise((resolve, reject) => {
    const img = new Image();
    const imagePath = `/characters/${voiceName}.png`;
    
    img.onload = () => {
      resolve({ url: imagePath, img });
    };
    
    img.onerror = () => {
      console.warn(`Failed to load image for ${voiceName}, using placeholder`);
      // Fallback to colored placeholder if image fails to load
      resolve(generateDefaultCharacterImage(voiceName.toUpperCase(), voiceName));
    };
    
    img.src = imagePath;
  });
}
function generateDefaultCharacterImage(characterName, voiceName) {
  const canvas = document.createElement('canvas');
  canvas.width = 1080;
  canvas.height = 1920;
  const ctx = canvas.getContext('2d');
  
  const theme = VOICE_THEMES[voiceName] || VOICE_THEMES.shawn;
  
  // Draw background
  ctx.fillStyle = theme.bg;
  ctx.fillRect(0, 0, 512, 512);
  
  // Draw character name
  ctx.fillStyle = '#FFFFFF';
  ctx.font = 'bold 72px system-ui, -apple-system, sans-serif';
  ctx.textAlign = 'center';
  ctx.textBaseline = 'middle';
  ctx.fillText(characterName, 256, 256);
  
  // Convert to image
  const img = new Image();
  img.src = canvas.toDataURL();
  return { url: img.src, img };
}

/**
 * Strip character tags from script for TTS generation
 * [ALEX]: Hello ‚Üí Hello
 */
function stripCharacterTags(scriptText) {
  if (!scriptText) return '';
  const lines = scriptText.split('\n');
  return lines
    .map(line => {
      const match = line.match(/^\[([A-Z][A-Z\s\-]*)\]:\s*(.+)$/i);
      return match ? match[2].trim() : line;
    })
    .join('\n');
}

function wrapCaption(ctx, text, maxWidth) {
  const words = (text || '').split(' ').filter(Boolean);
  const lines = [];
  let cur = '';
  for (const w of words) {
    const test = cur ? cur + ' ' + w : w;
    const m = ctx.measureText(test);
    if (m.width <= maxWidth || !cur) cur = test;
    else {
      lines.push(cur);
      cur = w;
    }
  }
  if (cur) lines.push(cur);
  return lines;
}

function roundedRectPath(g, x, y, w, h, r) {
  const rr = Math.min(r, w / 2, h / 2);
  g.beginPath();
  g.moveTo(x + rr, y);
  g.arcTo(x + w, y, x + w, y + rr, rr);
  g.arcTo(x + w, y + h, x + w - rr, y + h, rr);
  g.arcTo(x, y + h, x, y + h - rr, rr);
  g.arcTo(x, y, x + rr, y, rr);
  g.closePath();
}

function roundedRectFill(g, x, y, w, h, r) {
  roundedRectPath(g, x, y, w, h, r);
  g.fill();
}

function pickRecorderMime() {
  const cands = ['video/webm;codecs=vp9,opus', 'video/webm;codecs=vp8,opus', 'video/webm'];
  for (const t of cands) {
    try {
      if (window.MediaRecorder?.isTypeSupported?.(t)) return t;
    } catch {}
  }
}

function hexToRgb(hex) {
  const m = /^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(hex);
  if (!m) return { r: 0, g: 0, b: 0 };
  return { r: parseInt(m[1], 16), g: parseInt(m[2], 16), b: parseInt(m[3], 16) };
}

function mixHex(a, b, t) {
  const A = hexToRgb(a), B = hexToRgb(b);
  const r = Math.round(A.r + (B.r - A.r) * t);
  const g = Math.round(A.g + (B.g - A.g) * t);
  const bl = Math.round(A.b + (B.b - A.b) * t);
  return `rgb(${r},${g},${bl})`;
}

function gradientAtTime(t, dur, startIdx) {
  if (!dur || !isFinite(dur)) return PRESETS[startIdx];
  const total = PRESETS.length;
  const progress = Math.min(Math.max(t / dur, 0), 1) * total;
  const i0 = Math.floor(progress) % total;
  const i1 = (i0 + 1) % total;
  const frac = progress - Math.floor(progress);
  const [a0, a1] = PRESETS[i0];
  const [b0, b1] = PRESETS[i1];
  return [mixHex(a0, b0, frac), mixHex(a1, b1, frac)];
}

function drawImageCoverRounded(ctx, img, dx, dy, dW, dH, radius = 28, opacity = 1) {
  ctx.save();
  roundedRectPath(ctx, dx, dy, dW, dH, radius);
  ctx.clip();
  const iW = img.width, iH = img.height;
  if (!iW || !iH) {
    ctx.restore();
    return;
  }
  const scale = Math.max(dW / iW, dH / iH);
  const rW = iW * scale, rH = iH * scale;
  const x = dx + (dW - rW) / 2;
  const y = dy + (dH - rH) / 2;
  ctx.globalAlpha = Math.max(0, Math.min(1, opacity));
  ctx.drawImage(img, x, y, rW, rH);
  ctx.restore();
}

function buildSegmentsFromTextAndDuration(text, durationSec) {
  const words = text.trim().split(/\s+/).filter(Boolean);
  if (!words.length) return [{ start: 0, end: Math.max(1, durationSec), text: '' }];
  const FIXED_WORDS_PER_SEGMENT = 12;
  const chunks = [];
  for (let i = 0; i < words.length; i += FIXED_WORDS_PER_SEGMENT) {
    chunks.push(words.slice(i, i + FIXED_WORDS_PER_SEGMENT).join(' '));
  }
  const per = durationSec / chunks.length;
  return chunks.map((t, i) => ({ start: i * per, end: (i + 1) * per, text: t }));
}

function slideForTime(t, totalDuration, slides) {
  if (!slides.length) return null;
  if (!isFinite(totalDuration) || totalDuration <= 0) return slides[0].img;
  const per = totalDuration / slides.length;
  const idx = Math.min(slides.length - 1, Math.floor(t / per));
  return slides[idx]?.img ?? null;
}

const EPS = 1e-3;
function segmentIndexAtTime(segs, t, holdGapSec = 0.05, leadInSec = 0.03, tailOutSec = 0.06) {
  if (!segs.length) return -1;
  if (!Number.isFinite(t) || t < 0) t = 0;
  for (let i = 0; i < segs.length; i++) {
    const s = segs[i];
    if (t >= s.start - leadInSec - EPS && t <= s.end + tailOutSec + EPS) return i;
    if (t < s.start - EPS) {
      const prev = i - 1;
      if (prev >= 0) {
        const gap = s.start - segs[prev].end;
        return gap <= holdGapSec + EPS ? prev : -1;
      }
      return -1;
    }
  }
  const last = segs[segs.length - 1];
  const tailGap = t - last.end;
  return tailGap <= holdGapSec + EPS ? segs.length - 1 : -1;
}

function getUserPlan(user) {
  if (!user) return 'free';
  const subscriptionPlan = user?.publicMetadata?.subscriptionPlan;
  const subscriptionStatus = user?.publicMetadata?.subscriptionStatus;
  if (subscriptionPlan === 'pro' && subscriptionStatus === 'active') return 'pro';
  return 'free';
}

export default function ClientPage() {
  const userPlan = 'free'; // No authentication - everyone gets free plan

  const [selectedFormat, setSelectedFormat] = useState('4:3');
  const FORMAT = FORMATS[selectedFormat];
  const WIDTH = FORMAT?.width || 1280;
  const HEIGHT = FORMAT?.height || 960;
  const CAP_TOP = 1200;
  const CAP_BOTTOM = HEIGHT - 96;
  const CAP_BOX_H = CAP_BOTTOM - CAP_TOP;
  const audioRef = useRef(null);
  const [audioUrl, setAudioUrl] = useState('');
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecRef = useRef(null);
  const recChunksRef = useRef([]);
  const [transcript, setTranscript] = useState('');
  const [segments, setSegments] = useState([]);
  const [currentIdx, setCurrentIdx] = useState(0);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [presetIdx, setPresetIdx] = useState(1);
  const [autoBg, setAutoBg] = useState(true);
  const [isExporting, setIsExporting] = useState(false);
  const [err, setErr] = useState(null);
  const [phase, setPhase] = useState('idle');
  const [renderPct, setRenderPct] = useState(0);
  const [exportSupported, setExportSupported] = useState(null);
  const [exportReason, setExportReason] = useState('');
  const [voices] = useState(['shawn', 'chuck', 'brittany', 'kaitlyn', 'boomer', 'sage', 'max', 'randy', 'coral']);
  const [ttsText, setTtsText] = useState('');
  const [ttsVoice, setTtsVoice] = useState('brittany');
  const [isTtsBusy, setIsTtsBusy] = useState(false);
  const [artworks, setArtworks] = useState([]);
  const [artOpacity, setArtOpacity] = useState(1);
  const [customBrandingText, setCustomBrandingText] = useState('');
  
  // Character-switching states (max 2 characters for PrimoScenarios)
  const [detectedCharacters, setDetectedCharacters] = useState([]);
  const speakerTimingsRef = useRef(null); // Store actual speaker timings from backend
  const [characterImages, setCharacterImages] = useState({}); // { "ALEX": {url, img}, "JAMIE": {url, img} }
  const [characterVoices, setCharacterVoices] = useState({}); // { "ALEX": "alloy", "JAMIE": "nova" }
  
  const [wakeLock, setWakeLock] = useState(null);
  const capMetricsMemoRef = useRef(null);

  function computeUniformCaptionMetrics(ctx, segs, fallbackText) {
    const maxWidth = WIDTH * 0.94;
    const texts = segs?.map((s) => (s.text || '').trim()).filter(Boolean) ?? [];
    if (!texts.length) texts.push((fallbackText || '').trim() || 'Record or upload audio');
    const sizeFor = (text) => {
      let lo = 56, hi = 220, best = 56;
      while (lo <= hi) {
        const mid = Math.floor((lo + hi) / 2);
        const lh = Math.round(mid * 1.14);
        ctx.font = `bold ${mid}px Inter, system-ui, -apple-system, Segoe UI, Roboto, sans-serif`;
        const lines = wrapCaption(ctx, text, maxWidth);
        const ok = lines.length <= MAX_LINES && (lines.length - 1) * lh <= CAP_BOX_H;
        if (ok) {
          best = mid;
          lo = mid + 2;
        } else {
          hi = mid - 2;
        }
      }
      return best;
    };
    let uniform = 220;
    for (const txt of texts) uniform = Math.min(uniform, sizeFor(txt));
    const lineHeight = Math.round(uniform * 1.14);
    return { size: uniform, lineHeight };
  }

  const startRecord = async () => {
    try {
      setErr(null);
      recChunksRef.current = [];
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const rec = new MediaRecorder(stream, { mimeType: 'audio/webm' });
      mediaRecRef.current = rec;
      rec.ondataavailable = (e) => { if (e.data?.size) recChunksRef.current.push(e.data); };
      rec.onstop = () => {
        const blob = new Blob(recChunksRef.current, { type: 'audio/webm' });
        setAudioUrl(URL.createObjectURL(blob));
      };
      rec.start();
      setIsRecording(true);
    } catch (e) {
      setErr(e?.message || 'Mic permission problem.');
    }
  };

  const stopRecord = () => {
    const rec = mediaRecRef.current;
    try {
      rec?.stop();
      rec?.stream.getTracks().forEach((t) => t.stop());
    } finally {
      mediaRecRef.current = null;
      setIsRecording(false);
    }
  };

  const onUploadAudio = async (f) => {
    if (!f) return;
    setErr(null);
    setAudioUrl(URL.createObjectURL(f));
  };

  async function fileToCanvasImageSource(f) {
    const bmp = await window.createImageBitmap?.(f).catch(() => null);
    if (bmp) return bmp;
    const img = new Image();
    img.src = URL.createObjectURL(f);
    await img.decode();
    return img;
  }

  const onUploadArtwork = async (files) => {
    if (!files || !files.length) return;
    setErr(null);
    let arr = Array.from(files).slice(0, 3);
    const leadingNum = (name) => {
      const m = name.trim().match(/^(\d{1,4})[\s._-]?/);
      return m ? parseInt(m[1], 10) : NaN;
    };
    if (arr.every((f) => !Number.isNaN(leadingNum(f.name)))) {
      arr.sort((a, b) => leadingNum(a.name) - leadingNum(b.name));
    }
    const items = [];
    for (const f of arr) {
      try {
        const img = await fileToCanvasImageSource(f);
        items.push({ url: URL.createObjectURL(f), img });
      } catch {}
    }
    setArtworks(items);
  };

  const clearArtwork = () => {
    artworks.forEach((a) => URL.revokeObjectURL(a.url));
    setArtworks([]);
  };

  const transcribe = async () => {
    try {
      setErr(null);
      setIsTranscribing(true);
      if (!audioUrl) throw new Error('No audio to transcribe.');
      const res = await fetch(audioUrl);
      const blob = await res.blob();
      const fd = new FormData();
      fd.append('file', new File([blob], 'audio.webm', { type: blob.type || 'audio/webm' }));
      const r = await fetch('/api/transcribe', { method: 'POST', body: fd });
      if (!r.ok) {
        let msg = await r.text();
        try { msg = (await r.json())?.error || msg; } catch {}
        throw new Error(msg || 'Transcription failed.');
      }
      const data = await r.json();
      setTranscript((data?.text || '').trim());
      const segs = data?.segments?.map((s) => ({ start: s.start, end: s.end, text: (s.text || '').trim() })) || [];
      const durGuess = segs[segs.length - 1]?.end ?? 0;
      setSegments(normalizeSegments(segs, durGuess));
      setCurrentIdx(0);
      capMetricsMemoRef.current = null;
    } catch (e) {
      setErr(e?.message || 'Transcription failed.');
    } finally {
      setIsTranscribing(false);
    }
  };

  const generateTTS = async () => {
    try {
      if (!ttsText.trim()) return;
      const limit = PLAN_LIMITS[userPlan];
      if (ttsText.trim().length > limit.maxChars) {
        setErr(`Text exceeds ${limit.displayName} plan limit of ${limit.maxChars} characters.`);
        return;
      }
      setIsTtsBusy(true);
      setErr(null);
      
      let tt;
      
      if (detectedCharacters.length > 0) {
        // MULTI-VOICE MODE: Build segments with character voices
        const parsed = parseCharacterScript(ttsText.trim());
        const segments = parsed.lines.map(line => {
          const customVoice = characterVoices[line.speaker] || 'shawn';
          const apiVoice = VOICE_API_MAPPING[customVoice] || 'alloy';
          return {
            text: line.text,
            voice: apiVoice // Convert to API voice name
          };
        });
        
        console.log(`Generating multi-voice TTS with ${segments.length} segments`);
        
        tt = await fetch('/api/tts', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ 
            segments: segments, 
            userPlan: userPlan 
          }),
        });
      } else {
        // SINGLE VOICE MODE: Original behavior
        const apiVoice = VOICE_API_MAPPING[ttsVoice] || 'alloy';
        tt = await fetch('/api/tts', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ 
            text: ttsText.trim(), 
            voice: apiVoice, // Convert to API voice name
            format: 'mp3', 
            userPlan: userPlan 
          }),
        });
      }
      if (!tt.ok) {
        let msg = await tt.text();
        try { msg = (await tt.json())?.error || msg; } catch {}
        throw new Error(msg || 'TTS failed.');
      }
      
      // Capture speaker timing data from backend
      const speakerTimingsHeader = tt.headers.get('X-Speaker-Timings');
      const speakerTimings = speakerTimingsHeader ? JSON.parse(speakerTimingsHeader) : null;
      if (speakerTimings) {
        console.log('Received speaker timings from backend:', speakerTimings);
        speakerTimingsRef.current = speakerTimings;
      }
      
      const audioBlob = await tt.blob();
      const url = URL.createObjectURL(audioBlob);
      setAudioUrl(url);
      let dur = 10;
      try {
        const probe = new Audio(url);
        await new Promise((res, rej) => {
          probe.addEventListener('loadedmetadata', () => {
            if (isFinite(probe.duration) && probe.duration > 0) dur = probe.duration;
            res();
          }, { once: true });
          probe.addEventListener('error', () => rej(new Error('probe failed')), { once: true });
          probe.load();
        });
      } catch {}
      const fd = new FormData();
      fd.append('file', new File([audioBlob], 'tts.mp3', { type: audioBlob.type || 'audio/mpeg' }));
      const transcribeResponse = await fetch('/api/transcribe', { method: 'POST', body: fd });
      if (!transcribeResponse.ok) {
        let msg = await transcribeResponse.text();
        try { msg = (await transcribeResponse.json())?.error || msg; } catch {}
        throw new Error(msg || 'Transcription of TTS failed.');
      }
      const transcribeData = await transcribeResponse.json();
      const whisperSegs = transcribeData?.segments?.map((s) => ({ start: s.start, end: s.end, text: (s.text || '').trim() })) || [];
      const finalSegs = normalizeSegments(whisperSegs.length ? whisperSegs : buildSegmentsFromTextAndDuration(ttsText.trim(), dur), dur);
      setTranscript(transcribeData?.text || ttsText.trim());
      setSegments(finalSegs);
      setCurrentIdx(0);
      capMetricsMemoRef.current = null;
    } catch (e) {
      setErr(e?.message || 'TTS failed.');
    } finally {
      setIsTtsBusy(false);
    }
  };

  useEffect(() => {
    try {
      const saved = localStorage.getItem(VOICE_STORAGE_KEY);
      if (saved && voices.includes(saved)) setTtsVoice(saved);
    } catch {}
  }, []);

  useEffect(() => {
    try { localStorage.setItem(VOICE_STORAGE_KEY, ttsVoice); } catch {}
  }, [ttsVoice]);

  useEffect(() => {
    const a = audioRef.current;
    if (!a) return;
    const onTime = () => {
      if (!segments.length) return;
      const t = a.currentTime;
      const idx = segmentIndexAtTime(segments, t);
      setCurrentIdx((prev) => (idx === -1 ? prev : idx));
    };
    a.addEventListener('timeupdate', onTime);
    a.addEventListener('seeked', onTime);
    return () => {
      a.removeEventListener('timeupdate', onTime);
      a.removeEventListener('seeked', onTime);
    };
  }, [segments]);

  useEffect(() => { capMetricsMemoRef.current = null; }, [segments, transcript, selectedFormat]);

  // Auto-detect characters from tagged script (limit to 2 for PrimoScenarios)
  useEffect(() => {
    if (ttsText.trim()) {
      const parsed = parseCharacterScript(ttsText);
      const chars = parsed.characters.slice(0, 2); // Limit to 2 characters
      setDetectedCharacters(chars);
      
      // Auto-assign voices based on character names (1-to-1 match)
      // If character name doesn't match a voice, use next unused voice
      const newVoices = {};
      const usedVoices = new Set();
      
      // First pass: assign exact matches
      chars.forEach((char) => {
        const voiceName = char.toLowerCase();
        if (voices.includes(voiceName)) {
          newVoices[char] = voiceName;
          usedVoices.add(voiceName);
        }
      });
      
      // Second pass: assign unused voices to non-matching characters
      chars.forEach((char) => {
        if (!newVoices[char]) {
          // Find first unused voice
          const unusedVoice = voices.find(v => !usedVoices.has(v));
          newVoices[char] = unusedVoice || 'shawn'; // Fallback to shawn if all used
          usedVoices.add(newVoices[char]);
          console.log(`Character ${char} doesn't match a voice. Assigned ${newVoices[char]}.`);
        }
      });
      
      setCharacterVoices(newVoices);
      
      // Load actual character images from public folder
      const loadImages = async () => {
        const newImages = {};
        for (const char of chars) {
          const voice = newVoices[char] || 'shawn';
          try {
            newImages[char] = await loadCharacterImage(voice);
          } catch (err) {
            console.error(`Failed to load image for ${char}:`, err);
            // Fallback to placeholder
            newImages[char] = generateDefaultCharacterImage(char, voice);
          }
        }
        setCharacterImages(newImages);
      };
      
      loadImages();
    } else {
      setDetectedCharacters([]);
    }
  }, [ttsText]);

  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;
    const requestWakeLock = async () => {
      if ('wakeLock' in navigator) {
        try {
          const lock = await navigator.wakeLock.request('screen');
          setWakeLock(lock);
          lock.addEventListener('release', () => setWakeLock(null));
        } catch (e) { console.log('Wake lock failed:', e); }
      }
    };
    const releaseWakeLock = () => {
      if (wakeLock) {
        wakeLock.release();
        setWakeLock(null);
      }
    };
    audio.addEventListener('play', requestWakeLock);
    audio.addEventListener('pause', releaseWakeLock);
    audio.addEventListener('ended', releaseWakeLock);
    return () => {
      audio.removeEventListener('play', requestWakeLock);
      audio.removeEventListener('pause', releaseWakeLock);
      audio.removeEventListener('ended', releaseWakeLock);
      releaseWakeLock();
    };
  }, [audioUrl, wakeLock]);

  useEffect(() => {
    const a = audioRef.current;
    if (a && audioUrl) {
      try {
        if (!a.paused) a.pause();
        a.src = audioUrl;
        a.muted = false;
        a.volume = 1;
        a.load();
      } catch (e) { console.warn('audio reload failed:', e); }
    }
  }, [audioUrl]);

  useEffect(() => {
    const res = (() => {
      const hasCanvasCapture = typeof HTMLCanvasElement !== 'undefined' && typeof HTMLCanvasElement.prototype.captureStream === 'function';
      const hasMR = typeof window !== 'undefined' && 'MediaRecorder' in window;
      if (!hasCanvasCapture || !hasMR) return { ok: false, reason: 'Missing canvas.captureStream or MediaRecorder.' };
      const isTypeSupported = window.MediaRecorder?.isTypeSupported?.bind(window.MediaRecorder);
      const candidates = ['video/webm;codecs=vp9,opus', 'video/webm;codecs=vp8,opus', 'video/webm'];
      if (!!isTypeSupported && candidates.some((c) => isTypeSupported(c))) return { ok: true };
      return { ok: false, reason: 'MediaRecorder present but no compatible WebM profile.' };
    })();
    setExportSupported(res.ok);
    setExportReason(res.reason || '');
  }, []);

  /**
   * Map segments to speakers using actual timing from backend when available
   * Falls back to estimation if timing data not provided
   */
  function mapSegmentsToSpeakers(segments, scriptText, speakerTimings = null) {
    if (!scriptText || !segments.length) return segments;
    
    const parsed = parseCharacterScript(scriptText);
    if (!parsed.lines.length) return segments;
    
    // If we have actual timing from backend, use it!
    if (speakerTimings && speakerTimings.length > 0) {
      console.log('Using actual speaker timings from backend for perfect sync');
      
      return segments.map(segment => {
        const segmentMidpoint = (segment.start + segment.end) / 2;
        
        // Find which timing this segment falls into
        const timing = speakerTimings.find(t => 
          segmentMidpoint >= t.startTime && segmentMidpoint < t.endTime
        );
        
        const speaker = timing?.speaker || parsed.characters[0] || null;
        
        return {
          ...segment,
          speaker
        };
      });
    }
    
    // FALLBACK: Estimate timing (old approach)
    console.log('Estimating speaker timings (no backend data available)');
    
    const PAUSE_DURATION = 1.0;
    const INITIAL_SILENCE = 0.2;
    const totalAudioDuration = segments[segments.length - 1]?.end || 0;
    const totalScriptLength = parsed.lines.reduce((sum, line) => sum + line.text.length, 0);
    
    let speakerChanges = 0;
    for (let i = 1; i < parsed.lines.length; i++) {
      if (parsed.lines[i].speaker !== parsed.lines[i - 1].speaker) {
        speakerChanges++;
      }
    }
    
    const totalPauseTime = speakerChanges * PAUSE_DURATION;
    const totalSpeakingTime = Math.max(0, totalAudioDuration - totalPauseTime - INITIAL_SILENCE);
    
    const scriptTimeline = [];
    let cumulativeTime = 0.2;
    let lastSpeaker = null;
    
    for (const line of parsed.lines) {
      if (lastSpeaker && lastSpeaker !== line.speaker) {
        cumulativeTime += PAUSE_DURATION;
      }
      
      const lineDuration = (line.text.length / totalScriptLength) * totalSpeakingTime;
      scriptTimeline.push({
        speaker: line.speaker,
        startTime: cumulativeTime,
        endTime: cumulativeTime + lineDuration
      });
      cumulativeTime += lineDuration;
      lastSpeaker = line.speaker;
    }
    
    return segments.map(segment => {
      const segmentMidpoint = (segment.start + segment.end) / 2;
      
      const scriptLine = scriptTimeline.find(line => 
        segmentMidpoint >= line.startTime && segmentMidpoint < line.endTime
      );
      
      const speaker = scriptLine?.speaker || parsed.characters[0] || null;
      
      return {
        ...segment,
        speaker
      };
    });
  }

  // PRIMO SCENARIOS - Character-Switching Frame Renderer
  function drawFrame(ctx, t, grad, segs, transcriptText, bars, art, artOp, plan, customText, watermarkLogo) {
    // Background gradient
    const g = ctx.createLinearGradient(0, 0, 0, HEIGHT);
    g.addColorStop(0, grad[0]);
    g.addColorStop(1, grad[1]);
    ctx.fillStyle = g;
    ctx.fillRect(0, 0, WIDTH, HEIGHT);

    // Determine if we have character mode
    const hasCharacters = detectedCharacters.length > 0;
    
    if (hasCharacters) {
      const idx = segmentIndexAtTime(segs, t);
      const currentSeg = idx === -1 ? null : segs[idx];
      const speaker = currentSeg?.speaker || detectedCharacters[0];
      const characterImg = characterImages[speaker]?.img;
      
      const splitX = WIDTH / 2;
      
      // Draw character image on left half
      if (characterImg && characterImg.complete) {
        drawImageCoverRounded(ctx, characterImg, 0, 0, splitX, HEIGHT, 0, 1);
      }
      
      // Draw caption on right half - USE FULL HEIGHT
      if (currentSeg?.text) {
        const capX = splitX;
        const capW = WIDTH - splitX;
        
        // NEW: Use 90% of vertical space instead of small box
        const capTopMargin = HEIGHT * 0.05;  // 5% top margin
        const capBottomMargin = HEIGHT * 0.05;  // 5% bottom margin
        const capH = HEIGHT - capTopMargin - capBottomMargin;
        const capY = capTopMargin;
        
        // Increased max width to use more horizontal space too
        const maxWidth = capW * 0.85;  // Was 0.9, now 0.85 for better readability
        
        if (!capMetricsMemoRef.current) {
          capMetricsMemoRef.current = computeUniformCaptionMetrics(ctx, segs, transcriptText);
        }
        const { size: CAP_SIZE, lineHeight: CAP_LH } = capMetricsMemoRef.current;
        
        ctx.textAlign = 'left';  // Changed from 'center' - left align looks better for longer text
        ctx.textBaseline = 'top';  // Changed from 'middle'
        ctx.font = `bold ${CAP_SIZE}px Inter, system-ui, -apple-system, Segoe UI, Roboto, sans-serif`;
        ctx.fillStyle = '#fff';
        ctx.shadowColor = 'rgba(0, 0, 0, 0.8)';
        ctx.shadowBlur = 12;
        ctx.shadowOffsetX = 0;
        ctx.shadowOffsetY = 2;
        
        const lines = wrapCaption(ctx, currentSeg.text, maxWidth);  // Removed .slice(0, MAX_LINES) - show all lines
        
        // Draw from top with padding
        const textStartY = capY + 325;  // 325px from top
        
        for (let i = 0; i < lines.length; i++) {
          const y = textStartY + i * CAP_LH;
          // Left-aligned text with left margin
          const textX = capX + (capW * 0.075);  // 7.5% left margin
          ctx.fillText(lines[i], textX, y);
        }
        
        ctx.shadowColor = 'transparent';
        ctx.shadowBlur = 0;
      }
    
    } else {
      // LEGACY MODE: Original AudioGraffiti layout
      const left = WIDTH * 0.06, right = WIDTH * 0.94;
      const availW = right - left, gap = 10;
      const bins = Math.min(64, bars?.length || 64);
      const barW = (availW - (bins - 1) * gap) / bins;
      const maxBarH = 150;
      const midY = CAP_TOP - 120;
      
      // Artwork rendering
      const artTop = 120;
      const artBottom = midY - maxBarH / 2 - 60;
      const artHeight = Math.max(0, artBottom - artTop);

      if (art && artHeight > 40) {
        drawImageCoverRounded(ctx, art, left, artTop, availW, artHeight, 28, artOp);
      }

      // Waveform bars
      if (bars?.length) {
        ctx.fillStyle = '#f5c445';
        for (let i = 0; i < bins; i++) {
          const v = Math.max(0.08, Math.min(1, bars[i]));
          const h = v * maxBarH;
          const x = left + i * (barW + gap);
          const y = midY - h / 2;
          roundedRectFill(ctx, x, y, barW, h, 14);
        }
      }

      // Captions with proper timing
      const idx = segmentIndexAtTime(segs, t);
      const raw = idx === -1 ? '' : (segs[idx]?.text || transcriptText || 'Record or upload audio').trim();

      if (raw) {
        const maxWidth = WIDTH * 0.94;
        if (!capMetricsMemoRef.current) {
          capMetricsMemoRef.current = computeUniformCaptionMetrics(ctx, segs, transcriptText);
        }
        const { size: CAP_SIZE, lineHeight: CAP_LH } = capMetricsMemoRef.current;
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        ctx.font = `bold ${CAP_SIZE}px Inter, system-ui, -apple-system, Segoe UI, Roboto, sans-serif`;
        ctx.fillStyle = '#fff';
        const lines = wrapCaption(ctx, raw, maxWidth).slice(0, MAX_LINES);
        const blockH = (lines.length - 1) * CAP_LH;
        const startY = CAP_TOP + (CAP_BOX_H - blockH) / 2;
        for (let i = 0; i < lines.length; i++) {
          const y = startY + i * CAP_LH;
          ctx.fillText(lines[i], WIDTH / 2, y);
        }
      }
    }
 // Watermark - Logo image (smaller size)
if (plan === 'free' && watermarkLogo && watermarkLogo.complete) {
  ctx.save();
  
  const logoWidth = WIDTH * 0.23; // Reduced from 0.28 to 0.18 (18% of video width)
  const logoHeight = logoWidth * 0.45; // Maintains aspect ratio
  const logoX = WIDTH - logoWidth - 15; // 15px padding from right
  const logoY = 15; // 15px padding from top
  
  ctx.drawImage(watermarkLogo, logoX, logoY, logoWidth, logoHeight);
  
  ctx.restore();
}
  }

  async function renderWebMBlob(onProgress) {
    if (!audioUrl) throw new Error('No audio.');

    // LOAD WATERMARK LOGO HERE (NEW - INSERT THIS)
  const watermarkLogo = await new Promise((resolve) => {
    const img = new Image();
    img.src = '/scenaryoze-logo.png';
    img.onload = () => resolve(img);
    img.onerror = () => resolve(null);
  });


    const a = new Audio(audioUrl);
    a.crossOrigin = 'anonymous';
    a.preload = 'auto';
    await new Promise((res, rej) => {
      a.addEventListener('canplay', res, { once: true });
      a.addEventListener('error', rej, { once: true });
      a.load();
    });
    const totalDuration = a.duration;
    if (!totalDuration || !isFinite(totalDuration)) throw new Error('Could not determine audio duration.');

    const off = document.createElement('canvas');
    off.width = WIDTH;
    off.height = HEIGHT;
    const ctx = off.getContext('2d');

    const AC = window.AudioContext || window.webkitAudioContext;
    const ac = new AC();
    const src = ac.createMediaElementSource(a);
    const analyser = ac.createAnalyser();
    analyser.fftSize = 1024;
    analyser.smoothingTimeConstant = 0.6;
    const dest = ac.createMediaStreamDestination();
    src.connect(analyser);
    analyser.connect(dest);

    const videoStream = off.captureStream(FPS);
    const mixed = new MediaStream([...videoStream.getVideoTracks(), ...dest.stream.getAudioTracks()]);
    const mime = pickRecorderMime();
    
    // Add bitrate limits to keep file size reasonable (under 100MB for most videos)
    const recorderOptions = {
      mimeType: mime,
      videoBitsPerSecond: 3000000,  // 3 Mbps video = good quality, small file
      audioBitsPerSecond: 128000     // 128 kbps audio = standard quality
    };
    
    const rec = mime ? new MediaRecorder(mixed, recorderOptions) : new MediaRecorder(mixed);
    const chunks = [];
    rec.ondataavailable = (e) => { if (e.data.size) chunks.push(e.data); };
    const done = new Promise((resolve) => { rec.onstop = () => resolve(new Blob(chunks, { type: 'video/webm' })); });
    
    const computeBars = (() => {
      const fft = new Uint8Array(analyser.frequencyBinCount);
      const BINS = 64;
      const smooth = new Float32Array(BINS);
      let rollingMax = 0.35;
      const decay = 0.965;
      return () => {
        analyser.getByteFrequencyData(fft);
        const n = fft.length;
        let tickMax = 0;
        const out = new Array(BINS);
        for (let i = 0; i < BINS; i++) {
          const start = Math.floor(Math.pow(i / BINS, 2) * n);
          const end = Math.max(start + 1, Math.floor(Math.pow((i + 1) / BINS, 2) * n));
          let sum = 0, c = 0;
          for (let k = start; k < end; k++) { sum += fft[k]; c++; }
          const avg = (sum / Math.max(1, c)) / 255;
          if (avg > tickMax) tickMax = avg;
          const target = Math.pow(avg, 0.85);
          smooth[i] = smooth[i] * 0.6 + target * 0.4;
          out[i] = smooth[i];
        }
        rollingMax = Math.max(rollingMax * decay, tickMax);
        for (let i = 0; i < BINS; i++) {
          out[i] = out[i] / Math.max(0.18, rollingMax);
          out[i] = Math.min(1, Math.max(0.06, out[i]));
        }
        return out;
      };
    })();

    let segs = segments.length ? segments : [{ start: 0, end: totalDuration, text: transcript || '' }];
    
    // Map segments to speakers for character switching
    if (detectedCharacters.length > 0) {
      segs = mapSegmentsToSpeakers(segs, ttsText, speakerTimingsRef.current);
    }
    
    rec.start();
    a.currentTime = 0;
    await ac.resume();
    // Add 200ms delay before starting audio to prevent glitch
    await new Promise(resolve => setTimeout(resolve, 200));
    await a.play();

    let raf = 0;
    let fallbackInterval = null;
    
    const tick = () => {
      const currentTime = a.currentTime || 0;
      const b = computeBars();
      const grad = autoBg ? gradientAtTime(currentTime, totalDuration, presetIdx) : PRESETS[presetIdx];
      const slide = slideForTime(currentTime, totalDuration, artworks.map((x) => ({ img: x.img })));
      
      // Always render with 'free' plan for launch consistency
      drawFrame(ctx, currentTime, grad, segs, transcript, b, slide, artOpacity, 'free', customBrandingText, watermarkLogo);

      const progress = Math.min(currentTime / totalDuration, 1);
      onProgress?.(Math.min(99, Math.floor(progress * 99)));

      if (currentTime >= totalDuration) {
        if (fallbackInterval) clearInterval(fallbackInterval);
        cancelAnimationFrame(raf);
        rec.stop();
        return;
      }
      if (!document.hidden) raf = requestAnimationFrame(tick);
    };

    const onVis = () => {
      if (document.hidden) {
        if (!fallbackInterval) fallbackInterval = setInterval(tick, 250);
        cancelAnimationFrame(raf);
      } else {
        if (fallbackInterval) { clearInterval(fallbackInterval); fallbackInterval = null; }
        raf = requestAnimationFrame(tick);
      }
    };
    document.addEventListener('visibilitychange', onVis);
    raf = requestAnimationFrame(tick);

    const webm = await done;
    document.removeEventListener('visibilitychange', onVis);
    onProgress?.(100);
    src.disconnect(); analyser.disconnect(); dest.disconnect(); ac.close();
    if (webm.size < 1024) throw new Error('Captured video is empty; please record longer audio.');
    return webm;
  }

  const exportMP4 = async () => {
    let exportLock = null;
    try {
      if (exportSupported === false) {
        setErr('Export not supported in this browser. Try desktop Chrome/Edge/Firefox.');
        return;
      }
      try { exportLock = await navigator?.wakeLock?.request?.('screen'); } catch {}
      setErr(null);
      setIsExporting(true);
      setPhase('render');
      setRenderPct(0);
      const webm = await renderWebMBlob((p) => setRenderPct(p));
      const fd = new FormData();
      fd.append('file', webm, 'in.webm');
      setPhase('encode');
      const r = await fetch('/api/convert-mp4', { method: 'POST', body: fd });
      if (!r.ok) {
        let msg = `HTTP ${r.status} ${r.statusText}`;
        try {
          const payload = await r.json();
          msg = payload?.error || (payload.stderrTail ? `ffmpeg: ${payload.stderrTail}` : msg);
        } catch {}
        setErr(msg);
        return;
      }
      setPhase('save');
      const mp4 = await r.blob();
      const url = URL.createObjectURL(mp4);
      const a = document.createElement('a');
      a.href = url;
      a.download = `audiograffiti-${FORMAT.name.toLowerCase().replace(/[^a-z0-9]+/g, '-')}.mp4`;
      document.body.appendChild(a);
      a.click();
      a.remove();
      URL.revokeObjectURL(url);
    } catch (e) {
      setErr(e?.message || 'MP4 export failed');
    } finally {
      try { await exportLock?.release(); } catch {}
      setIsExporting(false);
      setPhase('idle');
      setRenderPct(0);
      if (audioRef.current && audioUrl) {
        const t = audioRef.current.currentTime || 0;
        audioRef.current.src = audioUrl;
        audioRef.current.currentTime = t;
        audioRef.current.muted = false;
        audioRef.current.volume = 1;
      }
    }
  };

  return (
    <div className="min-h-dvh w-full bg-[radial-gradient(ellipse_at_center,rgba(0,0,0,.9),#000)] text-white flex items-center justify-center p-4">
      <div className="w-[900px] max-w-[95vw] rounded-2xl bg-white/5 backdrop-blur-sm shadow-2xl border border-white/10 p-6">
        <div className="mb-4">
          <div className="text-xl font-bold text-white text-center">Scenaryoze</div>
        </div>

        {exportSupported === false && (
          <div className="mb-4 rounded-lg border border-yellow-400/40 bg-yellow-500/10 text-yellow-100 p-3 text-sm">
            <div className="font-medium">Export not supported in this browser.</div>
            <div className="mt-1 opacity-80">Please use <b>desktop Chrome, Edge, or Firefox</b>.</div>
            {exportReason && <div className="mt-1 opacity-60 text-xs">{exportReason}</div>}
          </div>
        )}

        <div className="mb-4">
          <div className="text-sm font-medium mb-2 text-white/90">
            Character Images
          </div>
          
          
            <div className="space-y-3">
              {detectedCharacters.length > 2 && (
                <div className="text-xs text-yellow-400 bg-yellow-500/10 border border-yellow-400/30 rounded-lg p-2">
                  ‚ö†Ô∏è Scenaryoze supports max 2 characters. Using first 2.
                </div>
              )}
              {detectedCharacters.map((character, idx) => (
                <div key={character} className="p-3 rounded-lg bg-black/20 border border-white/10">
                  <div className="flex items-start gap-3">
                    {/* Character Image Preview */}
                    <div className="relative h-16 w-16 rounded-md overflow-hidden border border-white/20 flex-shrink-0">
                      {characterImages[character] ? (
                        <img 
                          src={characterImages[character].url} 
                          alt={character} 
                          className="h-full w-full object-cover" 
                        />
                      ) : (
                        <div className="h-full w-full flex items-center justify-center bg-gray-700 text-white text-xs">
                          Loading...
                        </div>
                      )}
                    </div>
                    
                    {/* Character Info */}
                    <div className="flex-1 space-y-1">
                      <div className="font-medium text-white/90">{character}</div>
                      <div className="text-xs text-white/60">
                        Voice: {VOICE_THEMES[characterVoices[character]]?.name || characterVoices[character]}
                      </div>
                    </div>
                  </div>
                </div>
              ))}
            </div>
        </div>

        <div className="mb-4">
          <div className="text-sm font-medium mb-2 text-white/90">
            Character Script
            <span className="ml-2 text-xs text-white/50 font-normal">
              (Format: [NAME]:, NAME:, or **NAME:** dialogue)
            </span>
          </div>
          <textarea 
            value={ttsText} 
            onChange={(e) => setTtsText(e.target.value)} 
            rows={8} 
            className="w-full rounded-lg bg-white/10 border border-white/15 p-3 text-sm text-white placeholder-white/50 resize-none font-mono" 
            placeholder="[SHAWN]: Welcome to the helpdesk, how can I help you?&#10;[BRITTANY]: I can't access my email account.&#10;[SHAWN]: Let me check your account status first."
            maxLength={PLAN_LIMITS[userPlan].maxChars} 
          />
          <div className="flex items-center gap-2 mt-2">
            <button onClick={generateTTS} disabled={isTtsBusy || !ttsText.trim() || ttsText.trim().length > PLAN_LIMITS[userPlan].maxChars || detectedCharacters.length === 0} className="px-4 py-2 rounded-lg text-sm bg-green-500/90 hover:bg-green-500 text-black border border-green-300 font-medium disabled:opacity-60 disabled:cursor-not-allowed">
              {isTtsBusy ? 'Generating Audio‚Ä¶' : 'Generate Audio'}
            </button>
            <div className="text-xs ml-auto">
              <div className={`${ttsText.trim().length > PLAN_LIMITS[userPlan].maxChars ? 'text-red-400' : 'text-white/60'}`}>
                {ttsText.trim().length} / {PLAN_LIMITS[userPlan].maxChars}
              </div>
              <div className="text-white/50 text-[10px]">{userPlan} limit</div>
            </div>
          </div>
        </div>

        <div className="mb-4">
          <div className="text-sm font-medium mb-2 text-white/90">Background</div>
          <div className="flex gap-2 items-center">
            {PRESETS.map((g, i) => (
              <button key={i} onClick={() => setPresetIdx(i)} className={`h-6 w-8 rounded-md border transition-all ${presetIdx === i ? 'border-white/80 scale-110' : 'border-white/20 hover:border-white/40'}`} style={{ background: `linear-gradient(180deg, ${g[0]}, ${g[1]})` }} />
            ))}
            <div className="flex items-center gap-2 ml-auto text-xs text-white/80">
              <span>Auto</span>
              <button onClick={() => setAutoBg((v) => !v)} className={`px-2 py-1 rounded transition-colors ${autoBg ? 'bg-yellow-500/90 text-black font-medium' : 'bg-white/15 hover:bg-white/25 text-white/90'}`}>
                {autoBg ? 'On' : 'Off'}
              </button>
            </div>
          </div>
        </div>

        <div className="mb-4">
          <button onClick={exportMP4} disabled={isExporting || exportSupported === false || !segments.length} className="w-full px-4 py-3 rounded-lg bg-green-500/90 hover:bg-green-500 text-black text-lg font-bold disabled:opacity-60 disabled:cursor-not-allowed border border-green-300" title={!segments.length ? 'Transcribe audio first' : undefined}>
            Export MP4 Video
          </button>
          {isExporting && (
            <div className="mt-2 text-sm text-white/70 text-center">
              {phase === 'render' ? `Rendering‚Ä¶ ${renderPct}%` : 'Encoding on server‚Ä¶ (can take a minute)'}
            </div>
          )}
        </div>

        <div className="p-3 rounded-lg bg-black/20 border border-white/10">
          <audio ref={audioRef} src={audioUrl || undefined} controls playsInline preload="auto" className="w-full mb-2" />
          <div className="flex justify-between text-xs text-white/60">
            <div>Segments: {segments.length}</div>
            <div>Current: {segments.length ? `${Math.min(currentIdx + 1, segments.length)}/${segments.length}` : '‚Äî'}</div>
          </div>
        </div>

        {err && (
          <div className="mt-3 text-sm text-red-300 bg-red-900/30 rounded-lg p-3 border border-red-400/30 whitespace-pre-wrap">
            {err}
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="eslint.config.mjs">
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
  {
    ignores: [
      "node_modules/**",
      ".next/**",
      "out/**",
      "build/**",
      "next-env.d.ts",
    ],
  },
];

export default eslintConfig;
</file>

<file path="next.config.js">
/** @type {import('next').NextConfig} */
const nextConfig = {
  eslint: {
    ignoreDuringBuilds: true,
  },
  typescript: {
    ignoreBuildErrors: true,
  },
}

module.exports = nextConfig
</file>

<file path="nixpacks.toml">
[providers]
nodejs = "18"

[phases.setup]
nixPkgs = ["nodejs-18_x", "npm", "ffmpeg"]

[phases.install]
cmd = "npm ci"

[phases.build]
cmd = "npm run build"

[phases.start]
cmd = "npm start"

[variables]
NODE_ENV = "production"
</file>

<file path="postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path="railway.json">
{
    "build": {
      "builder": "NIXPACKS"
    },
    "deploy": {
      "startCommand": "npm start",
      "healthcheckPath": "/",
      "healthcheckTimeout": 300
    }
  }
</file>

<file path="README.md">
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
</file>

<file path="tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}
</file>

<file path="app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}
</file>

<file path="app/chatgpt/page.jsx">
'use client'

import { useState, useRef, useEffect } from 'react'

export default function AudioGraffiti() {
  const [appState, setAppState] = useState('recording')
  const [isRecording, setIsRecording] = useState(false)
  const [audioBlob, setAudioBlob] = useState(null)
  const [selectedStyle, setSelectedStyle] = useState('')
  const [videoUrl, setVideoUrl] = useState('')
  const [transcript, setTranscript] = useState('')
  const [audioUrl, setAudioUrl] = useState('')
  
  const mediaRecorderRef = useRef(null)
  const audioChunksRef = useRef([])
  const canvasRef = useRef(null)
  const audioRef = useRef(null)

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      const mediaRecorder = new MediaRecorder(stream)
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' })
        setAudioBlob(audioBlob)
        setAudioUrl(URL.createObjectURL(audioBlob))
        setAppState('selecting-style')
      }

      mediaRecorder.start()
      setIsRecording(true)
    } catch (err) {
      console.error('Error accessing microphone:', err)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop())
      setIsRecording(false)
    }
  }

  const selectStyle = (style) => {
    setSelectedStyle(style)
    generateVideo(style)
  }

  const generateVideo = async (style) => {
    if (!audioBlob) return
    
    setAppState('generating')
    
    const formData = new FormData()
    formData.append('audio', audioBlob)
    formData.append('style', style)
    
    try {
      const response = await fetch('/api/generate-video', {
        method: 'POST',
        body: formData
      })
      
      if (response.ok) {
        const data = await response.json()
        setTranscript(data.transcript)
        // Instead of setting videoUrl, we'll create the visual directly
        setAppState('sharing')
        setTimeout(() => {
          createAudioVisualization(style, data.transcript)
        }, 100)
      } else {
        console.error('Failed to generate video')
        setAppState('recording')
      }
    } catch (error) {
      console.error('Error generating video:', error)
      setAppState('recording')
    }
  }

  const createAudioVisualization = async (style, transcript) => {
    if (!canvasRef.current || !audioRef.current) return
    
    const canvas = canvasRef.current
    const ctx = canvas.getContext('2d')
    const audio = audioRef.current
    
    if (!ctx) return
    
    // Set canvas size
    canvas.width = 1080
    canvas.height = 1920
    
    // Create audio context for waveform analysis
    const audioContext = new (window.AudioContext || window.webkitAudioContext)()
    const analyser = audioContext.createAnalyser()
    const source = audioContext.createMediaElementSource(audio)
    
    source.connect(analyser)
    analyser.connect(audioContext.destination)
    
    analyser.fftSize = 256
    const bufferLength = analyser.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    
    const styleConfig = getStyleConfig(style)
    
    const animate = () => {
      analyser.getByteFrequencyData(dataArray)
      
      // Clear canvas with gradient background
      const gradient = ctx.createLinearGradient(0, 0, 0, canvas.height)
      gradient.addColorStop(0, styleConfig.background1)
      gradient.addColorStop(1, styleConfig.background2)
      ctx.fillStyle = gradient
      ctx.fillRect(0, 0, canvas.width, canvas.height)
      
      // Draw waveform
      const barWidth = canvas.width / bufferLength
      let x = 0
      
      for (let i = 0; i < bufferLength; i++) {
        const barHeight = (dataArray[i] / 255) * canvas.height * 0.5
        
        ctx.fillStyle = styleConfig.waveformColor
        ctx.fillRect(x, canvas.height / 2 - barHeight / 2, barWidth - 2, barHeight)
        
        x += barWidth
      }
      
      // Draw transcript
      ctx.fillStyle = styleConfig.textColor
      ctx.font = 'bold 48px Arial'
      ctx.textAlign = 'center'
      ctx.textBaseline = 'middle'
      
      const words = transcript.split(' ')
      const maxWidth = canvas.width - 100
      const lines = wrapText(ctx, transcript, maxWidth)
      const lineHeight = 60
      const totalHeight = lines.length * lineHeight
      const startY = canvas.height * 0.75 - totalHeight / 2
      
      lines.forEach((line, index) => {
        ctx.fillText(line, canvas.width / 2, startY + index * lineHeight)
      })
      
      // Draw branding
      ctx.fillStyle = styleConfig.textColor
      ctx.font = '24px Arial'
      ctx.textAlign = 'center'
      ctx.fillText('Created with AudioGraffiti', canvas.width / 2, canvas.height - 50)
      
      if (!audio.paused) {
        requestAnimationFrame(animate)
      }
    }
    
    audio.addEventListener('play', animate)
    audio.play()
  }

  const getStyleConfig = (style) => {
    switch (style.toLowerCase()) {
      case 'neon':
        return {
          background1: '#ff006e',
          background2: '#8338ec',
          waveformColor: '#00f5ff',
          textColor: '#ffffff'
        }
      case 'minimalist':
        return {
          background1: '#f8f9fa',
          background2: '#e9ecef',
          waveformColor: '#495057',
          textColor: '#212529'
        }
      default: // classic
        return {
          background1: '#667eea',
          background2: '#764ba2',
          waveformColor: '#ffffff',
          textColor: '#ffffff'
        }
    }
  }

  const wrapText = (ctx, text, maxWidth) => {
    const words = text.split(' ')
    const lines = []
    let currentLine = ''
    
    for (const word of words) {
      const testLine = currentLine + (currentLine ? ' ' : '') + word
      const metrics = ctx.measureText(testLine)
      
      if (metrics.width > maxWidth && currentLine) {
        lines.push(currentLine)
        currentLine = word
      } else {
        currentLine = testLine
      }
    }
    
    if (currentLine) {
      lines.push(currentLine)
    }
    
    return lines
  }

  const resetApp = () => {
    setAppState('recording')
    setIsRecording(false)
    setAudioBlob(null)
    setSelectedStyle('')
    setVideoUrl('')
    setTranscript('')
    setAudioUrl('')
  }

  const downloadCanvas = () => {
    if (!canvasRef.current) return
    
    const link = document.createElement('a')
    link.download = 'audiograffiti.png'
    link.href = canvasRef.current.toDataURL()
    link.click()
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-purple-900 via-blue-900 to-indigo-900 flex items-center justify-center p-4">
      <div className="max-w-md w-full bg-white/10 backdrop-blur-lg rounded-3xl p-8 shadow-2xl">
        
        {/* Header */}
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-white mb-2">AudioGraffiti</h1>
          <p className="text-blue-200 text-sm">Your voice, instantly visual</p>
        </div>

        {/* Recording State */}
        {appState === 'recording' && (
          <div className="text-center">
            <div className="mb-8">
              <div className={`w-32 h-32 mx-auto rounded-full border-4 flex items-center justify-center transition-all duration-300 ${
                isRecording 
                  ? 'border-red-400 bg-red-500/20 animate-pulse' 
                  : 'border-blue-400 bg-blue-500/20'
              }`}>
                <button
                  onClick={isRecording ? stopRecording : startRecording}
                  className={`w-16 h-16 rounded-full transition-all duration-300 ${
                    isRecording 
                      ? 'bg-red-500 hover:bg-red-600' 
                      : 'bg-blue-500 hover:bg-blue-600'
                  }`}
                >
                  {isRecording ? (
                    <div className="w-6 h-6 bg-white rounded-sm mx-auto"></div>
                  ) : (
                    <div className="w-0 h-0 border-l-[12px] border-l-white border-t-[8px] border-t-transparent border-b-[8px] border-b-transparent ml-1"></div>
                  )}
                </button>
              </div>
            </div>
            <p className="text-white text-lg mb-2">
              {isRecording ? 'Recording...' : 'Tap to record your thought'}
            </p>
            <p className="text-blue-200 text-sm">
              {isRecording ? 'Tap the red button when finished' : 'Say anything that comes to mind'}
            </p>
          </div>
        )}

        {/* Style Selection State */}
        {appState === 'selecting-style' && (
          <div>
            <h2 className="text-xl text-white font-semibold mb-6 text-center">Choose your style</h2>
            <div className="space-y-4">
              {['Classic', 'Neon', 'Minimalist'].map((style) => (
                <button
                  key={style}
                  onClick={() => selectStyle(style)}
                  className="w-full p-4 rounded-xl bg-white/10 hover:bg-white/20 transition-all duration-300 text-white font-medium border border-white/20 hover:border-white/40"
                >
                  {style}
                </button>
              ))}
            </div>
            <button
              onClick={resetApp}
              className="w-full mt-6 p-3 rounded-xl bg-gray-600/50 hover:bg-gray-600/70 transition-all duration-300 text-white"
            >
              Record Again
            </button>
          </div>
        )}

        {/* Generating State */}
        {appState === 'generating' && (
          <div className="text-center">
            <div className="w-16 h-16 mx-auto mb-6 border-4 border-blue-400 border-t-transparent rounded-full animate-spin"></div>
            <h2 className="text-xl text-white font-semibold mb-2">Creating your video...</h2>
            <p className="text-blue-200">This will take just a moment</p>
          </div>
        )}

        {/* Sharing State */}
        {appState === 'sharing' && (
          <div className="text-center">
            <h2 className="text-xl text-white font-semibold mb-6">Your video is ready!</h2>
            
            {/* Debug: Show transcript */}
            {transcript && (
              <div className="mb-4 p-3 bg-white/10 rounded-lg">
                <p className="text-sm text-blue-200">Transcript:</p>
                <p className="text-white text-sm">{transcript}</p>
              </div>
            )}
            
            {/* Live Canvas Visualization */}
            <div className="mb-6 bg-black rounded-xl overflow-hidden">
              <canvas 
                ref={canvasRef}
                className="w-full h-64 object-cover"
                style={{ aspectRatio: '9/16' }}
              />
              <audio 
                ref={audioRef}
                src={audioUrl}
                controls 
                className="w-full"
              />
            </div>
            
            <div className="space-y-3">
              <button 
                onClick={downloadCanvas}
                className="w-full p-3 rounded-xl bg-green-600 hover:bg-green-700 transition-all duration-300 text-white font-medium"
              >
                Download Image
              </button>
              <button className="w-full p-3 rounded-xl bg-blue-600 hover:bg-blue-700 transition-all duration-300 text-white font-medium">
                Share to Instagram
              </button>
              <button className="w-full p-3 rounded-xl bg-pink-600 hover:bg-pink-700 transition-all duration-300 text-white font-medium">
                Share to TikTok
              </button>
            </div>
            <button
              onClick={resetApp}
              className="w-full mt-6 p-3 rounded-xl bg-gray-600/50 hover:bg-gray-600/70 transition-all duration-300 text-white"
            >
              Create Another
            </button>
          </div>
        )}
      </div>
    </div>
  )
}
</file>

<file path="app/api/convert-mp4/route.js">
// app/api/convert-mp4/route.js
import { NextRequest, NextResponse } from "next/server";
import { spawn } from "child_process";
import { promises as fs } from "fs";
import path from "path";
import { createHash } from "crypto";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";
export const maxDuration = 300; // 5 minutes for video processing

/* ------------------ CONFIG ------------------ */
const TEMP_DIR = process.env.NODE_ENV === 'production' ? '/tmp' : path.join(process.cwd(), '.next', 'temp');
const MAX_FILE_SIZE = 100 * 1024 * 1024; // 100MB max file size
const CLEANUP_DELAY = 5000; // 5 seconds before cleanup

/* ------------------ UTILS ------------------ */
function generateTempId() {
  return createHash('md5').update(Date.now() + Math.random().toString()).digest('hex').slice(0, 12);
}

async function ensureTempDir() {
  try {
    await fs.mkdir(TEMP_DIR, { recursive: true });
  } catch (error) {
    console.warn('Temp directory creation failed:', error);
  }
}

async function cleanupFile(filePath) {
  try {
    await fs.unlink(filePath);
  } catch (error) {
    console.warn('File cleanup failed:', filePath, error);
  }
}

function findFFmpegPath() {
  // In production (Render), FFmpeg should be available in the system PATH
  if (process.env.NODE_ENV === 'production') {
    return 'ffmpeg';
  }
  // For local development, you might need to specify a path
  return 'ffmpeg';
}

async function convertWebMToMP4(inputPath, outputPath) {
  const ffmpegPath = findFFmpegPath();
  
  return new Promise((resolve) => {
    // --- CRITICAL FIX APPLIED HERE ---
    // Switched from 'medium' preset to 'ultrafast' to ensure completion within serverless limits.
    // Adjusted CRF to 28 to maintain reasonable quality and file size with the faster preset.
    const args = [
      '-i', inputPath,                    // Input file
      '-c:v', 'libx264',                  // Video codec
      '-c:a', 'aac',                      // Audio codec
      '-af', 'afade=t=in:st=0:d=0.3',     // 0.3 second audio fade-in
      '-preset', 'medium',                // Encoding speed vs quality balance
      '-crf', '23',                       // Quality setting (18-28 range)
      '-movflags', '+faststart',          // Web optimization
      '-y',                               // Overwrite output file
      outputPath                          // Output file
    ];

    console.log(`Starting FFmpeg conversion: ${ffmpegPath} ${args.join(' ')}`);
    
    const ffmpeg = spawn(ffmpegPath, args);
    let stderr = '';

    ffmpeg.stderr.on('data', (data) => {
      stderr += data.toString();
    });

    ffmpeg.on('close', (code) => {
      const success = code === 0;
      
      if (success) {
        console.log('FFmpeg conversion completed successfully');
      } else {
        console.error('FFmpeg conversion failed with code:', code);
        console.error('FFmpeg stderr:', stderr.slice(-1000)); // Log last 1000 chars of error
      }

      resolve({
        success,
        ffmpegPath,
        stderr: stderr.slice(-1000),
      });
    });

    ffmpeg.on('error', (error) => {
      console.error('FFmpeg spawn error:', error);
      resolve({
        success: false,
        ffmpegPath,
        stderr: `Spawn error: ${error.message}`,
      });
    });
  });
}

/* ------------------ HANDLER ------------------ */
export async function POST(req) {
  const tempId = generateTempId();
  let inputPath = null;
  let outputPath = null;

  try {
    await ensureTempDir();
    const formData = await req.formData();
    const file = formData.get('file');

    if (!file) {
      return NextResponse.json({ error: 'No file provided' }, { status: 400 });
    }

    if (file.size > MAX_FILE_SIZE) {
      return NextResponse.json({ error: `File too large. Max is ${MAX_FILE_SIZE / 1024 / 1024}MB` }, { status: 413 });
    }

    if (!file.type.includes('webm') && !file.name.toLowerCase().includes('webm')) {
        return NextResponse.json({ error: 'Only WebM files are supported' }, { status: 400 });
    }

    console.log(`Processing video conversion: ${file.name} (${file.size} bytes)`);

    inputPath = path.join(TEMP_DIR, `input_${tempId}.webm`);
    outputPath = path.join(TEMP_DIR, `output_${tempId}.mp4`);

    const buffer = Buffer.from(await file.arrayBuffer());
    await fs.writeFile(inputPath, buffer);

    console.log(`Input file written: ${inputPath}`);

    const conversionResult = await convertWebMToMP4(inputPath, outputPath);

    if (!conversionResult.success) {
      return NextResponse.json({
        error: 'Video conversion failed on the server',
        ffmpegPath: conversionResult.ffmpegPath,
        stderrTail: conversionResult.stderr,
      }, { status: 500 });
    }

    try {
      const outputStats = await fs.stat(outputPath);
      if (outputStats.size === 0) throw new Error('Output file is empty');
      console.log(`Conversion successful: ${outputStats.size} bytes`);
    } catch (error) {
      return NextResponse.json({
        error: 'Conversion succeeded but output file is invalid',
        stderrTail: conversionResult.stderr,
      }, { status: 500 });
    }

    const outputBuffer = await fs.readFile(outputPath);

    setTimeout(async () => {
      if (inputPath) await cleanupFile(inputPath);
      if (outputPath) await cleanupFile(outputPath);
    }, CLEANUP_DELAY);

    return new NextResponse(outputBuffer, {
      status: 200,
      headers: {
        'Content-Type': 'video/mp4',
        'Content-Disposition': 'attachment; filename="audiograffiti-export.mp4"',
      },
    });

  } catch (error) {
    console.error('Video conversion handler error:', {
      message: error.message,
      stack: error.stack,
      tempId,
    });

    if (inputPath) await cleanupFile(inputPath);
    if (outputPath) await cleanupFile(outputPath);

    return NextResponse.json({
      error: 'An unexpected error occurred during video conversion.',
    }, { status: 500 });
  }
}
</file>

<file path="app/api/tts/route.js">
// app/api/tts/route.js
// Scenaryoze - Fish Audio TTS (Single Provider)
import { NextRequest, NextResponse } from "next/server";
import { createHash } from "crypto";
import { promises as fs } from "fs";
import { spawn } from "child_process";
import path from "path";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";
export const maxDuration = 60;

/* ------------------ CONFIG ------------------ */
const DEFAULT_FORMAT = "mp3";
const DEFAULT_VOICE = "brittany";

const CACHE_DIR =
  (process.env.TTS_CACHE_DIR?.trim()) ||
  path.join(process.env.NODE_ENV === 'production' ? '/tmp' : process.cwd(), 
           process.env.NODE_ENV === 'production' ? 'tts-cache' : '.next/tts-cache');

const TEMP_DIR = process.env.NODE_ENV === 'production' ? '/tmp' : path.join(process.cwd(), '.next', 'temp');

const TTL_HOURS = Number(process.env.TTS_CACHE_TTL_HOURS || 720);
const TTL_MS = TTL_HOURS * 60 * 60 * 1000;

// Fish Audio API Configuration
const FISH_AUDIO_API_KEY = process.env.FISH_AUDIO_API_KEY;
if (!FISH_AUDIO_API_KEY) {
  console.error('‚ö†Ô∏è  FISH_AUDIO_API_KEY environment variable is required');
}

// Map character voice names to Fish Audio voice IDs
const FISH_AUDIO_VOICE_MAP = {
  'shawn': '536d3a5e000945adb7038665781a4aca',      // Ethan
  'chuck': 'ccbc13d6002a46b7883f607fd8fe0516',      // Black Man
  'max': '37a48fabcd8241ab9b69d8675fb1fe13',        // Brian
  'boomer': 'ba24f05b17644498adb77243afd11dd9',     // Mild Manager
  'randy': 'bf322df2096a46f18c579d0baa36f41d',      // Adrian
  'brittany': '2a9605eeafe84974b5b20628d42c0060',   // Female Voice
  'kaitlyn': 'da8ae28bb18d4a1ca55eccf096f4c8da',    // Black Woman
  'sage': '933563129e564b19a115bedd57b7406a',       // Sarah
  'coral': 'e107ce68d2a64e928c3a674781ce9d56'       // Upbeat Woman
};

const PLAN_LIMITS = {
  free: { maxChars: 50000, displayName: 'Free' },
  pro: { maxChars: 50000, displayName: 'Pro' }
};

/* ------------------ CHARACTER TAG STRIPPING ------------------ */
function stripCharacterTags(text) {
  // Remove character tags like [SHAWN]:, **BRITTANY:**, etc.
  // This allows Fish Audio to see emotion tags at the beginning of the sentence
  return text.replace(/^\s*\*?\*?\[?[A-Z][A-Z\s\-]*\]?\*?\*?:\s*/i, '').trim();
}

/* ------------------ FISH AUDIO TTS ------------------ */
async function generateFishAudioTTS(text, voiceId) {
  // Strip character tags before sending to Fish Audio
  const cleanText = stripCharacterTags(text);
  
  console.log(`Calling Fish Audio API with voice ${voiceId}`);
  console.log(`Original text: "${text.substring(0, 100)}..."`);
  console.log(`Cleaned text: "${cleanText.substring(0, 100)}..."`);
  
  const url = 'https://api.fish.audio/v1/tts';
  
  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${FISH_AUDIO_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      text: cleanText,       // Send cleaned text WITHOUT character tags
      reference_id: voiceId,
      model: 's1',           // Use Fish Audio S1 for emotion support
      format: 'mp3',
      normalize: false,      // CRITICAL: Preserve emotion tags!
      latency: 'normal'
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error(`Fish Audio API error: ${response.status} - ${errorText}`);
    throw new Error(`Fish Audio API error: ${response.status}`);
  }

  const audioBuffer = Buffer.from(await response.arrayBuffer());
  console.log(`Fish Audio generated ${audioBuffer.length} bytes`);
  
  return audioBuffer;
}

/* ------------------ UTILS ------------------ */
function normalizeSpaces(s) {
  return String(s || "").replace(/\s+/g, " ").trim();
}

function sha256(s) {
  return createHash("sha256").update(s).digest("hex");
}

function shardPath(root, hash, ext) {
  const a = hash.slice(0, 2) || "00";
  const b = hash.slice(2, 4) || "00";
  return path.join(root, a, b, `${hash}.${ext}`);
}

function generateTempId() {
  return createHash('md5')
    .update(Date.now() + Math.random().toString())
    .digest('hex')
    .slice(0, 12);
}

async function ensureDir(dir) {
  try {
    await fs.mkdir(dir, { recursive: true });
  } catch (error) {
    console.warn('Directory creation failed:', error);
  }
}

async function readFreshFileIfAny(p) {
  try {
    const st = await fs.stat(p);
    const age = Date.now() - st.mtimeMs;
    if (age <= TTL_MS && st.size > 0) {
      return await fs.readFile(p);
    }
  } catch {}
  return null;
}

async function writeCacheFile(filePath, data) {
  try {
    await ensureDir(path.dirname(filePath));
    await fs.writeFile(filePath, data);
  } catch (error) {
    console.warn('Cache write failed:', error);
  }
}

async function cleanupFile(filePath) {
  try {
    await fs.unlink(filePath);
  } catch (error) {
    console.warn('File cleanup failed:', filePath);
  }
}

function findFFmpegPath() {
  return process.env.NODE_ENV === 'production' ? 'ffmpeg' : 'ffmpeg';
}

/* ------------------ PLAN VALIDATION ------------------ */
function validateUserPlan(plan) {
  if (typeof plan === 'string' && (plan === 'free' || plan === 'pro')) {
    return plan;
  }
  return 'free';
}

/* ------------------ VALIDATION ------------------ */
function validateTTSRequest(payload) {
  const errors = [];
  
  if (!payload || typeof payload !== 'object') {
    errors.push('Request body must be a JSON object');
  }
  
  const hasSegments = Array.isArray(payload.segments);
  
  if (hasSegments) {
    if (payload.segments.length === 0) {
      errors.push('Segments array cannot be empty');
    }
    
    if (payload.segments.length > 50) {
      errors.push('Maximum 50 segments allowed');
    }
    
    for (const segment of payload.segments) {
      if (!segment.text || typeof segment.text !== 'string') {
        errors.push('Each segment must have a text property');
        break;
      }
      const normalized = normalizeSpaces(segment.text);
      if (!normalized) {
        errors.push('Segment text cannot be empty');
        break;
      }
      
      if (normalized.length > 4096) {
        errors.push(`Segment text exceeds 4096 characters`);
        break;
      }
      
      const voice = (segment.voice || DEFAULT_VOICE).trim();
      const validVoices = Object.keys(FISH_AUDIO_VOICE_MAP);
      if (!validVoices.includes(voice)) {
        errors.push(`Invalid voice in segment: ${voice}. Must be one of: ${validVoices.join(', ')}`);
        break;
      }
    }
    
    return { errors, isMultiVoice: true, segments: payload.segments };
  } else {
    const text = normalizeSpaces(payload.text || '');
    if (!text) {
      errors.push('Text parameter is required and cannot be empty');
    }
    
    if (text.length > 4096) {
      errors.push('Text cannot exceed 4096 characters');
    }
    
    const voice = (payload.voice || DEFAULT_VOICE).trim();
    const validVoices = Object.keys(FISH_AUDIO_VOICE_MAP);
    if (!validVoices.includes(voice)) {
      errors.push(`Voice must be one of: ${validVoices.join(', ')}`);
    }
    
    const format = (payload.format || DEFAULT_FORMAT).toLowerCase();
    if (!['mp3'].includes(format)) {
      errors.push('Format must be mp3');
    }
    
    return { errors, isMultiVoice: false, text, voice, format };
  }
}

/* ------------------ SILENCE GENERATION ------------------ */
async function generateSilenceFile(outputPath, durationSeconds) {
  const ffmpegPath = findFFmpegPath();
  
  return new Promise((resolve) => {
    const args = [
      '-f', 'lavfi',
      '-i', `anullsrc=r=44100:cl=stereo`,
      '-t', durationSeconds.toString(),
      '-c:a', 'libmp3lame',
      '-q:a', '2',
      '-y',
      outputPath
    ];

    const ffmpeg = spawn(ffmpegPath, args);
    let stderr = '';

    ffmpeg.stderr.on('data', (data) => {
      stderr += data.toString();
    });

    ffmpeg.on('close', (code) => {
      const success = code === 0;
      if (success) {
        console.log(`Generated ${durationSeconds}s silence file`);
      } else {
        console.error('Silence generation failed:', stderr.slice(-500));
      }
      resolve({ success, stderr: stderr.slice(-500) });
    });

    ffmpeg.on('error', (error) => {
      console.error('FFmpeg spawn error during silence generation:', error);
      resolve({ success: false, stderr: `Spawn error: ${error.message}` });
    });
  });
}

/* ------------------ AUDIO CONCATENATION ------------------ */
async function concatenateAudioFiles(inputFiles, outputPath) {
  const ffmpegPath = findFFmpegPath();
  
  return new Promise((resolve) => {
    const inputs = [];
    const filterComplex = [];
    
    inputFiles.forEach((file, i) => {
      inputs.push('-i', file);
      filterComplex.push(`[${i}:a]`);
    });
    
    const concatFilter = `${filterComplex.join('')}concat=n=${inputFiles.length}:v=0:a=1[temp];[temp]afade=t=in:st=0:d=0.1[outa]`;
    
    const args = [
      ...inputs,
      '-filter_complex', concatFilter,
      '-map', '[outa]',
      '-c:a', 'libmp3lame',
      '-q:a', '2',
      '-y',
      outputPath
    ];

    console.log(`Concatenating ${inputFiles.length} audio files with FFmpeg`);
    
    const ffmpeg = spawn(ffmpegPath, args);
    let stderr = '';

    ffmpeg.stderr.on('data', (data) => {
      stderr += data.toString();
    });

    ffmpeg.on('close', (code) => {
      const success = code === 0;
      
      if (success) {
        console.log('Audio concatenation completed successfully');
      } else {
        console.error('Audio concatenation failed with code:', code);
        console.error('FFmpeg stderr:', stderr.slice(-500));
      }

      resolve({ success, stderr: stderr.slice(-500) });
    });

    ffmpeg.on('error', (error) => {
      console.error('FFmpeg spawn error:', error);
      resolve({ success: false, stderr: `Spawn error: ${error.message}` });
    });
  });
}

/* ------------------ MULTI-VOICE GENERATION ------------------ */
async function generateMultiVoiceAudio(segments) {
  const tempId = generateTempId();
  await ensureDir(TEMP_DIR);
  
  const tempFiles = [];
  const outputPath = path.join(TEMP_DIR, `multivoice_${tempId}.mp3`);
  
  try {
    console.log(`Generating ${segments.length} voice segments with Fish Audio`);
    
    // Add 0.2s initial silence to prevent audio click
    const initialSilenceFile = path.join(TEMP_DIR, `initial_silence_${tempId}.mp3`);
    const initialSilenceResult = await generateSilenceFile(initialSilenceFile, 0.2);
    
    if (initialSilenceResult.success) {
      tempFiles.push(initialSilenceFile);
      console.log('Added 0.2s initial silence to prevent audio tick');
    }
    
    let lastVoice = null;
    
    for (let i = 0; i < segments.length; i++) {
      const segment = segments[i];
      const text = normalizeSpaces(segment.text);
      const voice = (segment.voice || DEFAULT_VOICE).trim();
      
      // Add 1.0s pause when voice changes
      if (i > 0 && lastVoice && lastVoice !== voice) {
        console.log(`Adding 0.3s pause between ${lastVoice} and ${voice}`);
        
        const pauseFile = path.join(TEMP_DIR, `pause_${tempId}_${i}.mp3`);
        const pauseResult = await generateSilenceFile(pauseFile, 0.3);
        
        if (pauseResult.success) {
          tempFiles.push(pauseFile);
        } else {
          console.warn('Failed to generate pause, continuing without it');
        }
      }
      
      // Map voice name to Fish Audio voice ID
      const voiceId = FISH_AUDIO_VOICE_MAP[voice.toLowerCase()];
      if (!voiceId) {
        throw new Error(`Unknown voice: ${voice}. Valid voices: ${Object.keys(FISH_AUDIO_VOICE_MAP).join(', ')}`);
      }
      
      console.log(`Segment ${i + 1}/${segments.length}: ${voice} - "${text.substring(0, 50)}..."`);
      
      // Generate TTS using Fish Audio - text includes emotion tags
      const audio = await generateFishAudioTTS(text, voiceId);
      
      if (audio.length < 100) {
        throw new Error(`Segment ${i + 1} generated suspiciously small audio`);
      }
      
      const tempFile = path.join(TEMP_DIR, `segment_${tempId}_${i}.mp3`);
      await fs.writeFile(tempFile, audio);
      tempFiles.push(tempFile);
      
      console.log(`Segment ${i + 1} generated: ${audio.length} bytes`);
      
      lastVoice = voice;
    }
    
    // Concatenate all segments
    console.log(`Concatenating ${tempFiles.length} files`);
    const concatResult = await concatenateAudioFiles(tempFiles, outputPath);
    
    if (!concatResult.success) {
      throw new Error(`Audio concatenation failed: ${concatResult.stderr}`);
    }
    
    // Read final audio
    const finalAudio = await fs.readFile(outputPath);
    
    if (finalAudio.length < 100) {
      throw new Error('Final concatenated audio is suspiciously small');
    }
    
    console.log(`Multi-voice audio generated successfully: ${finalAudio.length} bytes`);
    
    return { audio: finalAudio };
    
  } finally {
    // Cleanup temp files
    for (const file of tempFiles) {
      await cleanupFile(file);
    }
    await cleanupFile(outputPath);
  }
}

/* ------------------ SINGLE VOICE GENERATION ------------------ */
async function generateSingleVoiceAudio(text, voice, format, bypass, userId) {
  const key = sha256([userId, voice, format, text].join(":"));
  const filePath = shardPath(CACHE_DIR, key, format);

  // Try cache first (unless bypassed)
  if (!bypass) {
    const hit = await readFreshFileIfAny(filePath);
    if (hit) {
      console.log('Cache hit for single-voice TTS');
      return { audio: hit, cached: true };
    }
  }

  // Generate TTS with Fish Audio
  console.log(`Generating single-voice TTS with Fish Audio`);
  
  const voiceId = FISH_AUDIO_VOICE_MAP[voice];
  if (!voiceId) {
    throw new Error(`Unknown voice: ${voice}`);
  }
  
  const audio = await generateFishAudioTTS(text, voiceId);

  if (audio.length < 100) {
    throw new Error('Generated audio file is suspiciously small');
  }

  // Cache the result
  await writeCacheFile(filePath, audio);

  return { audio, cached: false };
}

/* ------------------ MAIN HANDLER ------------------ */
export async function POST(req) {
  const startTime = Date.now();
  
  try {
    if (!FISH_AUDIO_API_KEY) {
      console.error('TTS API called without Fish Audio API key configured');
      return NextResponse.json(
        { error: "TTS service is not properly configured" },
        { status: 500 }
      );
    }

    let payload;
    try {
      payload = await req.json();
    } catch {
      return NextResponse.json(
        { error: "Invalid JSON in request body" },
        { status: 400 }
      );
    }

    const validation = validateTTSRequest(payload);
    if (validation.errors.length > 0) {
      return NextResponse.json(
        { error: validation.errors.join('; ') },
        { status: 400 }
      );
    }

    const userId = (payload.userId || req.headers.get("x-user-id") || "anon").toString();
    const userPlan = validateUserPlan(payload.userPlan);
    
    const bypass =
      !!payload.bypassCache ||
      req.nextUrl.searchParams.get("bypass") === "1" ||
      req.headers.get("x-bypass-cache") === "1";

    let audio;
    let cached = false;
    const format = validation.isMultiVoice ? 'mp3' : (validation.format || DEFAULT_FORMAT);

    if (validation.isMultiVoice) {
      console.log(`Multi-voice TTS request: ${validation.segments.length} segments`);
      const result = await generateMultiVoiceAudio(validation.segments);
      audio = result.audio;
      cached = false;
    } else {
      const result = await generateSingleVoiceAudio(
        validation.text,
        validation.voice,
        format,
        bypass,
        userId
      );
      audio = result.audio;
      cached = result.cached;
    }

    const generationTime = Date.now() - startTime;
    console.log(`TTS completed: ${audio.length} bytes in ${generationTime}ms (${userPlan})`);

    return new NextResponse(audio, {
      status: 200,
      headers: {
        "Content-Type": "audio/mpeg",
        "Cache-Control": "public, max-age=3600",
        "X-Cache-Hit": cached ? "1" : "0",
        "X-TTS-Provider": "fish-audio",
        "X-User-Plan": userPlan,
        "X-Generation-Time": generationTime.toString(),
        "X-Multi-Voice": validation.isMultiVoice ? "1" : "0",
      },
    });

  } catch (error) {
    console.error('TTS generation failed:', {
      error: error.message,
      stack: error.stack,
      timestamp: new Date().toISOString(),
    });

    let userMessage = "TTS generation failed";
    let statusCode = 500;

    if (error.message?.includes('API key')) {
      userMessage = "TTS service configuration error";
      statusCode = 503;
    } else if (error.message?.includes('rate limit') || error.message?.includes('quota')) {
      userMessage = "TTS service is temporarily busy, please try again";
      statusCode = 429;
    } else if (error.message?.includes('timeout')) {
      userMessage = "TTS generation timed out, please try again";
      statusCode = 504;
    } else if (error.message?.includes('suspiciously small')) {
      userMessage = "Generated audio was invalid, please try different text";
      statusCode = 422;
    } else if (error.message?.includes('concatenation failed')) {
      userMessage = "Audio merging failed, please try again";
      statusCode = 500;
    }

    return NextResponse.json(
      { 
        error: userMessage,
        ...(process.env.NODE_ENV === 'development' && { 
          debug: error.message 
        })
      },
      { status: statusCode }
    );
  }
}
</file>

<file path="app/layout.jsx">
import "./globals.css";

export const metadata = {
  title: "Scenaryoze",
  description: "Character-switching training videos",
};

export default function RootLayout({ children }) {
  return (
    <html lang="en">
      <body>
        {children}
      </body>
    </html>
  );
}
</file>

<file path="app/api/transcribe/route.js">
// app/api/transcribe/route.js
import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";

export const runtime = "nodejs";
export const dynamic = "force-dynamic";
export const maxDuration = 60; // 1 minute timeout for transcription

/* ------------------ CONFIG ------------------ */
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const MAX_FILE_SIZE = 25 * 1024 * 1024; // 25MB limit
const MIN_FILE_SIZE = 1024; // 1KB minimum
const ALLOWED_TYPES = ['audio/mpeg', 'audio/wav', 'audio/webm', 'audio/mp4', 'audio/ogg'];

/* ------------------ HELPER FUNCTIONS ------------------ */
function validateAudioFile(file) {
  if (!file) {
    return { isValid: false, error: 'No audio file provided' };
  }

  if (file.size > MAX_FILE_SIZE) {
    return { isValid: false, error: `File too large. Maximum size is ${MAX_FILE_SIZE / 1024 / 1024}MB` };
  }

  if (file.size < MIN_FILE_SIZE) {
    return { isValid: false, error: 'File too small. Minimum size is 1KB' };
  }

  if (file.type && !ALLOWED_TYPES.includes(file.type)) {
    return { isValid: false, error: `Unsupported file type: ${file.type}. Supported types: ${ALLOWED_TYPES.join(', ')}` };
  }

  return { isValid: true };
}

function sanitizeFileName(fileName) {
  if (!fileName) return 'audio.webm';
  return fileName.replace(/[^a-zA-Z0-9.-]/g, '_').substring(0, 100);
}

/* ------------------ MAIN API HANDLER ------------------ */
export async function POST(req) {
  try {
    console.log('Transcription API called');

    // Validate environment
    if (!process.env.OPENAI_API_KEY) {
      console.error('Missing OpenAI API key');
      return NextResponse.json(
        { error: 'OpenAI API key not configured' },
        { status: 500 }
      );
    }

    // Parse form data
    let formData;
    try {
      formData = await req.formData();
    } catch (error) {
      console.error('Failed to parse form data:', error);
      return NextResponse.json(
        { error: 'Invalid form data' },
        { status: 400 }
      );
    }

    const file = formData.get('audio') || formData.get('file');
    if (!file) {
      console.error('No audio file in form data');
      return NextResponse.json(
        { error: 'No audio file provided' },
        { status: 400 }
      );
    }

    // Validate file
    const validation = validateAudioFile(file);
    if (!validation.isValid) {
      console.error('File validation failed:', validation.error);
      return NextResponse.json(
        { error: validation.error },
        { status: 400 }
      );
    }

    const sanitizedName = sanitizeFileName(file.name || 'audio.webm');
    console.log(`Transcription request: ${sanitizedName} (${file.size} bytes, ${file.type || 'unknown type'})`);

    // Convert file to buffer for OpenAI API
    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // Transcribe using OpenAI Whisper API
    let transcription;
    try {
      console.log(`Attempting transcription with file: ${sanitizedName}, size: ${buffer.length}, type: ${file.type}`);
      
      // Create form data for OpenAI API (more reliable than Blob for file uploads)
      const openaiFormData = new FormData();
      
      // Create a proper file-like object for OpenAI
      const fileForOpenAI = new Blob([buffer], { type: file.type || 'audio/mpeg' });
      
      openaiFormData.append('file', fileForOpenAI, sanitizedName);
      openaiFormData.append('model', 'whisper-1');
      openaiFormData.append('response_format', 'verbose_json');
      openaiFormData.append('timestamp_granularities[]', 'segment');

      // Make direct fetch request to OpenAI API instead of using client
      const openaiResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        },
        body: openaiFormData
      });

      if (!openaiResponse.ok) {
        const errorText = await openaiResponse.text();
        console.error(`OpenAI API error: ${openaiResponse.status} - ${errorText}`);
        throw new Error(`OpenAI API error: ${openaiResponse.status}`);
      }

      transcription = await openaiResponse.json();
      
      console.log(`Transcription successful: ${transcription.segments?.length || 0} segments`);
    } catch (apiError) {
      console.error('OpenAI API error:', {
        message: apiError.message,
        status: apiError.status,
        code: apiError.code
      });

      if (apiError.message?.includes('413') || apiError.status === 413) {
        return NextResponse.json(
          { error: 'Audio file too large for transcription' },
          { status: 413 }
        );
      }

      if (apiError.message?.includes('400') || apiError.status === 400) {
        return NextResponse.json(
          { error: 'Invalid audio format or corrupted file' },
          { status: 400 }
        );
      }

      return NextResponse.json(
        { error: 'Transcription service temporarily unavailable' },
        { status: 503 }
      );
    }

    // Process segments
    const segments = (transcription.segments || []).map((segment, index) => ({
      id: index,
      start: segment.start || 0,
      end: segment.end || 1,
      text: (segment.text || '').trim()
    })).filter(seg => seg.text.length > 0);

    console.log(`Processed ${segments.length} valid segments`);

    // Return successful response
    return NextResponse.json({
      success: true,
      text: transcription.text || '',
      segments: segments,
      duration: segments.length > 0 ? Math.max(...segments.map(s => s.end)) : 0,
      metadata: {
        fileSize: file.size,
        fileName: sanitizedName,
        segmentCount: segments.length
      }
    });

  } catch (error) {
    console.error('Transcription failed:', {
      error: error.message,
      stack: error.stack,
      timestamp: new Date().toISOString()
    });

    return NextResponse.json(
      { 
        error: 'Internal transcription error',
        details: process.env.NODE_ENV === 'development' ? error.message : undefined
      },
      { status: 500 }
    );
  }
}
</file>

<file path="package.json">
{
  "name": "kwyzzer",
  "version": "1.0.0",
  "private": true,
  "engines": {
    "node": "22.x"
  },
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "postbuild": "echo 'Build completed successfully'"
  },
  "dependencies": {
    "@clerk/nextjs": "^4.31.8",
    "@stripe/stripe-js": "^7.9.0",
    "autoprefixer": "^10.4.16",
    "next": "^14.2.25",
    "openai": "^4.24.7",
    "postcss": "^8.4.32",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "stripe": "^18.5.0",
    "tailwindcss": "^3.4.0"
  },
  "devDependencies": {
    "@types/node": "^20.10.6",
    "@types/react": "^18.2.46",
    "@types/react-dom": "^18.2.18",
    "eslint": "^8.56.0",
    "eslint-config-next": "14.0.4",
    "typescript": "^5.3.3"
  }
}
</file>

<file path="app/page.jsx">
import ClientPage from './client-page';

export const dynamic = 'force-dynamic';

export default function Page() {
  return <ClientPage />;
}
</file>

</files>
